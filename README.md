# Energy conserving neural closure models 

[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://tobyvg.github.io/ECNCM_1D.jl/stable/)
[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://tobyvg.github.io/ECNCM_1D.jl/dev/)
[![Build Status](https://github.com/tobyvg/ECNCM_1D.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/tobyvg/ECNCM_1D.jl/actions/workflows/CI.yml?query=branch%3Amain)
[![Coverage](https://codecov.io/gh/tobyvg/ECNCM_1D.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/tobyvg/ECNCM_1D.jl)

## Abstract 

In turbulence modeling, we are concerned with finding closure models that represent the effect of the unresolved subgrid scales on the resolved scales. Recent approaches gravitate towards machine learning techniques to construct such models. However, the stability of machine-learned closure models and their abidance by physical structure (e.g. symmetries, conservation laws)  are still open problems. To tackle both issues, we take the `discretize first, filter next' approach. In this approach we apply a spatial averaging filter to existing fine-grid discretizations. In this work, the main novelty is that we extend the filtered system of equations with a set of equations that dynamically model the energy of the subgrid scales. Having an estimate of the energy of the subgrid scales, we can use the concept of energy conservation to derive stability. The subgrid energy containing variables are determined via a data-driven technique. The closure model is used to model the interaction between the filtered quantities and the subgrid energy containing variables. Therefore the total energy should be conserved. Abiding by this conservation law would yield guaranteed stability of the system. In this work, we propose a novel skew-symmetric convolutional neural network architecture that satisfies this law. The result is that stability is guaranteed, independent of the weights and biases of the network. Importantly, as our framework allows energy exchange between resolved scales and subgrid scales it allows for backscatter. To model dissipative systems (e.g. viscous flows), the framework is extended with a diffusive component. The introduced neural network architecture is constructed such that it also satisfies momentum conservation. We apply the new methodology to both the viscous Burgers' equation and the Korteweg-De Vries equation in 1D and show superior stability properties when compared to a vanilla convolutional neural network.
